"""


æ ¸å¿ƒæŠ€æœ¯:
1. å¤šæ¨¡å‹é›†æˆæƒ…æ„Ÿåˆ†æ (Transformer + VADER + Lexicon)
2. çœŸæ­£çš„æ–¹é¢çº§æƒ…æ„Ÿåˆ†æ (ABSA) - ä¾å­˜å¥æ³• + è§„åˆ™
3. åŠ¨æ€ä¸»é¢˜å»ºæ¨¡ (LDA/NMF + æ—¶é—´æ¼”åŒ–)
4. è¯„è®ºè´¨é‡è¯„ä¼°
5. å‘½åå®ä½“è¯†åˆ«
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict# æ•°æ®ç±»replaceå­—å…¸
from enum import Enum
from collections import Counter, defaultdict#ä¸€äº›é«˜çº§å®¹å™¨å“¦
import re
import logging#æ—¥å¿—ç³»ç»Ÿ
from functools import lru_cache
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

#æ ‡ç­¾æƒ…æ„Ÿæšä¸¾çš„éƒ¨åˆ†ï¼
class SentimentLabel(Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"


@dataclass#è£…é¥°å™¨
class SentimentResult:
    label: SentimentLabel
    score: float  # 0-1, è¶Šé«˜è¶Šæ­£é¢ï¼
    confidence: float
    method: str
    details: Dict = field(default_factory=dict)#å·¥å‚å‡½æ•°


@dataclass#åˆ†æç»“æœå®¹å™¨ï¼
class AspectSentiment:
    aspect: str
    aspect_cn: str  
    sentiment: SentimentLabel
    confidence: float
    mentions: int
    evidence: List[str] = field(default_factory=list)#å¯è§£é‡Šæ€§


@dataclass#æ¨¡å‹ç»“æœå®¹å™¨
class TopicInfo:
    topic_id: int
    keywords: List[str]
    keyword_weights: List[float]
    num_docs: int
    label: str  # è‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾


@dataclass
class ReviewAnalysis:
    """å•æ¡è¯„è®ºçš„å®Œæ•´åˆ†æç»“æœ"""
    review_id: str
    sentiment: SentimentResult
    aspects: List[AspectSentiment]
    quality_score: float
    topics: List[int]
    entities: List[str]
    word_count: int
    
    def to_dict(self) -> Dict:#è½¬æˆå­—å…¸æ–¹ä¾¿å±•ç¤º#èšåˆ#åºåˆ—åŒ–
        return {
            'review_id': self.review_id,
            'sentiment_label': self.sentiment.label.value,
            'sentiment_score': self.sentiment.score,
            'sentiment_confidence': self.sentiment.confidence,
            'quality_score': self.quality_score,
            'word_count': self.word_count,
            'aspects': [{'aspect': a.aspect, 'aspect_cn': a.aspect_cn, 
                        'sentiment': a.sentiment.value, 'confidence': a.confidence}
                       for a in self.aspects]
        }


class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self._init_resources()#å¯ä»¥è‡ªåŠ¨ä¸‹è½½å“¦
    
    def _init_resources(self):
        """åˆå§‹åŒ– NLTK èµ„æº"""
        import nltk
        import ssl
        
        # å¤„ç† SSL é—®é¢˜ (ğŸŒŸ)#è·¨å¹³å°å…¼å®¹
        try:
            _create_unverified_https_context = ssl._create_unverified_context
        except AttributeError:
            pass
        else:
            ssl._create_default_https_context = _create_unverified_https_context
        
        # ä¸‹è½½å¿…è¦èµ„æº
        for resource in ['punkt', 'punkt_tab', 'stopwords', 'wordnet', 
                        'vader_lexicon', 'averaged_perceptron_tagger']:
            try:
                nltk.data.find(f'tokenizers/{resource}' if 'punkt' in resource 
                             else f'corpora/{resource}' if resource in ['stopwords', 'wordnet']
                             else f'sentiment/{resource}' if resource == 'vader_lexicon'
                             else f'taggers/{resource}')
            except LookupError:
                nltk.download(resource, quiet=True)
        
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer
        
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        
        # ç”µå½±é¢†åŸŸä¿ç•™è¯
        self.domain_words = {
            'plot', 'acting', 'actor', 'actress', 'director', 'scene',
            'character', 'story', 'ending', 'script', 'dialogue',
            'cinematography', 'soundtrack', 'score', 'performance',
            'cast', 'movie', 'film', 'cinema', 'screen', 'visual',
            'effect', 'music', 'pacing', 'twist'
        }
        
        # å¦å®šè¯#æƒ…æ„Ÿææ€§åè½¬
        self.negation_words = {'not', "n't", 'never', 'no', 'none', 'neither',
                              'nobody', 'nothing', 'nowhere', 'hardly', 'barely'}
        
        # æ­£åˆ™è¡¨è¾¾å¼
        self.html_re = re.compile(r'<[^>]+>')
        self.url_re = re.compile(r'http\S+|www\S+')
        self.special_re = re.compile(r'[^a-zA-Z0-9\s\.\,\!\?\'\-]')#ä½†æ˜¯æˆ‘ä¿ç•™äº†å­—æ¯æ•°å­—æ ‡ç‚¹
        self.whitespace_re = re.compile(r'\s+')#è¿ç»­ç©ºæ ¼
    
    def clean(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        if not text or not isinstance(text, str):
            return ""
        text = self.html_re.sub('', text)
        text = self.url_re.sub('', text)
        text = self.special_re.sub(' ', text)
        text = self.whitespace_re.sub(' ', text).strip()
        return text
    
    def tokenize(self, text: str, remove_stopwords: bool = True, #æ™ºèƒ½åˆ†è¯
                lemmatize: bool = True) -> List[str]:
        """åˆ†è¯"""
        from nltk.tokenize import word_tokenize
        
        try:
            tokens = word_tokenize(text.lower())# NLTK åˆ†è¯#split
        except:
            tokens = text.lower().split()# é™çº§æ–¹æ¡ˆ
        
        result = []
        for token in tokens:
            # A. ä¿ç•™å¦å®šè¯é¢†åŸŸè¯ 
            if token in self.negation_words or token in self.domain_words:
                result.append(token)
            # B. å»é™¤åœç”¨è¯
            elif remove_stopwords and token in self.stop_words:
                continue
            elif len(token) < 2:
                continue
            # C. è¯å½¢è¿˜åŸ 
            elif lemmatize:
                result.append(self.lemmatizer.lemmatize(token))
            else:
                result.append(token)
        
        return result
    
    def extract_sentences(self, text: str) -> List[str]:
        """æå–å¥å­"""
        from nltk.tokenize import sent_tokenize
        try:
            return sent_tokenize(text)
        except:
            return [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]


class MultiModelSentimentAnalyzer:
    """
    å¤šæ¨¡å‹é›†æˆæƒ…æ„Ÿåˆ†æå™¨#æˆ‘è¦è®²è¿™ä¸ªç‚¹
    
    é›†æˆç­–ç•¥:
    1. Transformer (DistilBERT) - æ·±åº¦è¯­ä¹‰ç†è§£-0.5
    2. VADER - è§„åˆ™åŸºç¡€ï¼Œæ“…é•¿ç¤¾äº¤åª’ä½“æ–‡æœ¬-0.3
    3. é¢†åŸŸè¯å…¸ - ç”µå½±é¢†åŸŸç‰¹å®šè¯æ±‡-0.2
    4. åŠ æƒæŠ•ç¥¨å†³ç­–
    """
    
    def __init__(self, use_transformer: bool = True):
        self.preprocessor = TextPreprocessor()
        self.use_transformer = use_transformer
        self._transformer = None
        self._init_lexicons()# åˆå§‹åŒ–è¯å…¸
    
    def _init_lexicons(self):
        """åˆå§‹åŒ–æƒ…æ„Ÿè¯å…¸"""
        from nltk.sentiment import SentimentIntensityAnalyzer
        self.vader = SentimentIntensityAnalyzer()# åŠ è½½ VADER æ¨¡å‹
        
        # ç”µå½±é¢†åŸŸå¢å¼ºè¯å…¸
        self.positive_words = {
            'masterpiece': 2.0, 'brilliant': 1.8, 'outstanding': 1.7,
            'amazing': 1.6, 'excellent': 1.6, 'fantastic': 1.5,
            'wonderful': 1.5, 'incredible': 1.5, 'perfect': 1.8,
            'beautiful': 1.3, 'stunning': 1.4, 'captivating': 1.4,
            'compelling': 1.3, 'engaging': 1.2, 'gripping': 1.3,
            'riveting': 1.4, 'breathtaking': 1.5, 'heartwarming': 1.3,
            'hilarious': 1.4, 'touching': 1.3, 'powerful': 1.3,
            'unforgettable': 1.5, 'genius': 1.6, 'flawless': 1.7,
            'superb': 1.5, 'remarkable': 1.4, 'impressive': 1.3,
            'love': 1.2, 'loved': 1.2, 'best': 1.4, 'great': 1.1, 'good': 0.8
        }
        
        self.negative_words = {
            'terrible': -1.8, 'awful': -1.7, 'horrible': -1.7,
            'worst': -2.0, 'bad': -1.2, 'poor': -1.3, 'boring': -1.4,
            'disappointing': -1.5, 'waste': -1.4, 'stupid': -1.3,
            'ridiculous': -1.2, 'annoying': -1.2, 'dull': -1.3,
            'weak': -1.1, 'mediocre': -1.0, 'forgettable': -1.2,
            'predictable': -0.9, 'cliche': -1.0, 'overrated': -1.3,
            'confusing': -1.1, 'slow': -0.8, 'painful': -1.4,
            'unbearable': -1.6, 'disaster': -1.7, 'unwatchable': -1.8,
            'pretentious': -1.3, 'tedious': -1.4, 'lifeless': -1.5
        }
        
        # å¼ºåŒ–è¯å’Œå‡å¼±è¯
        self.intensifiers = {
            'very': 1.5, 'really': 1.4, 'extremely': 1.8, 'absolutely': 1.7,
            'completely': 1.6, 'totally': 1.5, 'utterly': 1.7, 'incredibly': 1.6,
            'so': 1.3, 'highly': 1.4, 'truly': 1.4, 'particularly': 1.3
        }
        
        self.diminishers = {
            'somewhat': 0.7, 'slightly': 0.6, 'barely': 0.5, 'hardly': 0.4,
            'kind of': 0.6, 'sort of': 0.6, 'a bit': 0.7, 'a little': 0.7
        }
    
    @property#æ‡’åŠ è½½
    def transformer(self):
        """æ‡’åŠ è½½ Transformer"""
        if self._transformer is None and self.use_transformer:
            try:
                from transformers import pipeline
                self._transformer = pipeline(
                    "sentiment-analysis",
                    model="distilbert-base-uncased-finetuned-sst-2-english",
                    device=-1,  # CPU
                    truncation=True,
                    max_length=512
                )
                logger.info("âœ“ Transformer æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.warning(f"Transformer åŠ è½½å¤±è´¥: {e}")
                self._transformer = False
        return self._transformer
    
    def _analyze_transformer(self, text: str) -> Optional[SentimentResult]:
        """Transformer åˆ†æ"""
        if not self.transformer:
            return None
        
        try:
            result = self.transformer(text[:512])[0]
            is_positive = result['label'] == 'POSITIVE'
            return SentimentResult(
                label=SentimentLabel.POSITIVE if is_positive else SentimentLabel.NEGATIVE,
                score=result['score'] if is_positive else 1 - result['score'],
                confidence=result['score'],
                method='transformer'
            )
        except Exception as e:
            logger.warning(f"Transformer åˆ†æå¤±è´¥: {e}")
            return None
    
    def _analyze_vader(self, text: str) -> SentimentResult:
        """VADER åˆ†æ"""
        scores = self.vader.polarity_scores(text)
        compound = scores['compound']
        
        if compound >= 0.05:
            label = SentimentLabel.POSITIVE
        elif compound <= -0.05:
            label = SentimentLabel.NEGATIVE
        else:
            label = SentimentLabel.NEUTRAL
        
        return SentimentResult(
            label=label,
            score=(compound + 1) / 2,
            confidence=abs(compound),
            method='vader',
            details=scores
        )
    
    def _analyze_lexicon(self, text: str) -> SentimentResult:
        """è¯å…¸åˆ†æ"""
        tokens = self.preprocessor.tokenize(text, remove_stopwords=False, lemmatize=False)
        
        total_score = 0.0
        word_count = 0
        
        for i, token in enumerate(tokens):
            # æ£€æŸ¥ä¿®é¥°è¯
            modifier = 1.0
            if i > 0:
                prev = tokens[i-1]
                modifier = self.intensifiers.get(prev, self.diminishers.get(prev, 1.0))
            
            # æ£€æŸ¥å¦å®š
            negated = any(neg in tokens[max(0, i-3):i] for neg in self.preprocessor.negation_words)
            
            # è®¡ç®—åˆ†æ•°
            if token in self.positive_words:
                score = self.positive_words[token] * modifier
                total_score += -score if negated else score
                word_count += 1
            elif token in self.negative_words:
                score = self.negative_words[token] * modifier
                total_score += -score if negated else score
                word_count += 1
        
        if word_count == 0:
            return SentimentResult(
                label=SentimentLabel.NEUTRAL,
                score=0.5,
                confidence=0.0,
                method='lexicon'
            )
        
        avg_score = total_score / word_count
        normalized = (avg_score + 2) / 4  # å½’ä¸€åŒ–åˆ° 0-1
        normalized = max(0, min(1, normalized))
        
        if normalized > 0.55:
            label = SentimentLabel.POSITIVE
        elif normalized < 0.45:
            label = SentimentLabel.NEGATIVE
        else:
            label = SentimentLabel.NEUTRAL
        
        return SentimentResult(
            label=label,
            score=normalized,
            confidence=abs(normalized - 0.5) * 2,
            method='lexicon',
            details={'word_count': word_count, 'raw_score': total_score}
        )
    
    def analyze(self, text: str, method: str = 'ensemble') -> SentimentResult:
        """
        åˆ†ææƒ…æ„Ÿ
        
        method: 'transformer', 'vader', 'lexicon', 'ensemble'
        """
        if not text or len(text.strip()) < 10:
            return SentimentResult(
                label=SentimentLabel.NEUTRAL,
                score=0.5,
                confidence=0.0,
                method='default'
            )
        
        if method == 'transformer':
            result = self._analyze_transformer(text)
            return result if result else self._analyze_vader(text)
        elif method == 'vader':
            return self._analyze_vader(text)
        elif method == 'lexicon':
            return self._analyze_lexicon(text)
        else:  # ensemble
            return self._analyze_ensemble(text)
    
    def _analyze_ensemble(self, text: str) -> SentimentResult:
        """é›†æˆåˆ†æ - åŠ æƒæŠ•ç¥¨"""
        results = []
        weights = []
        
        # Transformer 
        trans = self._analyze_transformer(text)
        if trans:
            results.append(trans)
            weights.append(0.5)
        
        # VADER
        vader = self._analyze_vader(text)
        results.append(vader)
        weights.append(0.3 if trans else 0.5)
        
        # Lexicon
        lexicon = self._analyze_lexicon(text)
        results.append(lexicon)
        weights.append(0.2 if trans else 0.5)
        
        # åŠ æƒæŠ•ç¥¨
        label_scores = defaultdict(float)
        for r, w in zip(results, weights):
            label_scores[r.label] += w * r.confidence
        
        final_label = max(label_scores, key=label_scores.get)
        final_confidence = label_scores[final_label] / sum(weights)
        
        # åŠ æƒå¹³å‡åˆ†æ•°
        avg_score = sum(r.score * w for r, w in zip(results, weights)) / sum(weights)
        
        return SentimentResult(
            label=final_label,
            score=avg_score,
            confidence=final_confidence,
            method='ensemble',
            details={
                'transformer': trans.label.value if trans else None,
                'vader': vader.label.value,
                'lexicon': lexicon.label.value,
                'weights': weights
            }
        )
    
    def batch_analyze(self, texts: List[str], method: str = 'ensemble',
                     show_progress: bool = True) -> List[SentimentResult]:
        """æ‰¹é‡åˆ†æ"""
        from tqdm import tqdm
        iterator = tqdm(texts, desc="æƒ…æ„Ÿåˆ†æ") if show_progress else texts
        return [self.analyze(text, method) for text in iterator]


class AspectBasedAnalyzer:
    """
    æ–¹é¢çº§æƒ…æ„Ÿåˆ†æ (ABSA)
    
    ç”µå½±è¯„è®ºæ–¹é¢:#å›¾è°±
    - acting (æ¼”æŠ€)
    - plot (å‰§æƒ…)  
    - direction (å¯¼æ¼”)
    - cinematography (æ‘„å½±)
    - soundtrack (é…ä¹)
    - dialogue (å¯¹ç™½)
    - pacing (èŠ‚å¥)
    - ending (ç»“å±€)
    - effects (ç‰¹æ•ˆ)
    - characters (è§’è‰²)
    """
    
    ASPECTS = {
        'acting': {
            'cn': 'æ¼”æŠ€',
            'keywords': ['acting', 'actor', 'actress', 'performance', 'perform', 
                        'cast', 'role', 'portray', 'played', 'star'],
            'weight': 1.0
        },
        'plot': {
            'cn': 'å‰§æƒ…',
            'keywords': ['plot', 'story', 'storyline', 'narrative', 'twist',
                        'premise', 'tale', 'script', 'writing', 'written'],
            'weight': 1.0
        },
        'direction': {
            'cn': 'å¯¼æ¼”',
            'keywords': ['director', 'direction', 'directing', 'directed',
                        'filmmaker', 'vision', 'helm', 'nolan', 'spielberg'],
            'weight': 0.9
        },
        'cinematography': {
            'cn': 'æ‘„å½±',
            'keywords': ['cinematography', 'camera', 'visual', 'shot', 'shots',
                        'photography', 'frame', 'framing', 'lens', 'lighting'],
            'weight': 0.8
        },
        'soundtrack': {
            'cn': 'é…ä¹',
            'keywords': ['music', 'soundtrack', 'score', 'sound', 'audio',
                        'song', 'composer', 'hans zimmer', 'theme', 'musical'],
            'weight': 0.9
        },
        'dialogue': {
            'cn': 'å¯¹ç™½',
            'keywords': ['dialogue', 'dialog', 'line', 'lines', 'conversation',
                        'speech', 'talking', 'quote', 'memorable'],
            'weight': 0.7
        },
        'pacing': {
            'cn': 'èŠ‚å¥',
            'keywords': ['pace', 'pacing', 'slow', 'fast', 'runtime', 'length',
                        'drag', 'dragged', 'tempo', 'speed', 'boring', 'long'],
            'weight': 0.8
        },
        'ending': {
            'cn': 'ç»“å±€',
            'keywords': ['ending', 'end', 'conclusion', 'finale', 'climax',
                        'resolution', 'final', 'last', 'finish'],
            'weight': 0.9
        },
        'effects': {
            'cn': 'ç‰¹æ•ˆ',
            'keywords': ['effects', 'cgi', 'vfx', 'special effects', 'animation',
                        'action', 'stunts', 'practical', 'visual effects'],
            'weight': 0.8
        },
        'characters': {
            'cn': 'è§’è‰²',
            'keywords': ['character', 'characters', 'protagonist', 'antagonist',
                        'villain', 'hero', 'development', 'arc', 'depth'],
            'weight': 0.9
        }
    }
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.sentiment_analyzer = MultiModelSentimentAnalyzer(use_transformer=False)
    #æ¯ä¸€ä¸ªéƒ½è·‘bretä¼šæ­»äºº
    def analyze(self, text: str) -> List[AspectSentiment]:
        """åˆ†æå•æ¡è¯„è®ºçš„æ–¹é¢æƒ…æ„Ÿ"""
        if not text:
            return []
        #æ‹†
        sentences = self.preprocessor.extract_sentences(text)
        aspect_data = defaultdict(lambda: {'sentences': [], 'sentiments': []})
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            #éå†æ‰€æœ‰æ–¹é¢
            for aspect_key, aspect_info in self.ASPECTS.items():
                matched = [kw for kw in aspect_info['keywords'] if kw in sentence_lower]
                #å¼€å§‹åŒ¹é…ç¯èŠ‚ï¼
                if matched:
                    sentiment = self.sentiment_analyzer.analyze(sentence, method='lexicon')
                    aspect_data[aspect_key]['sentences'].append(sentence)
                    aspect_data[aspect_key]['sentiments'].append(sentiment)
        
        results = []
        for aspect_key, data in aspect_data.items():
            if not data['sentiments']:
                continue
            
            # å¤šæ•°æŠ•ç¥¨å†³å®šæƒ…æ„Ÿ
            labels = [s.label for s in data['sentiments']]
            label_counts = Counter(labels)
            dominant = label_counts.most_common(1)[0][0]
            
            avg_confidence = np.mean([s.confidence for s in data['sentiments']])
            
            results.append(AspectSentiment(
                aspect=aspect_key,
                aspect_cn=self.ASPECTS[aspect_key]['cn'],
                sentiment=dominant,
                confidence=avg_confidence,
                mentions=len(data['sentiments']),
                evidence=data['sentences'][:3]
            ))
        
        return results
    
    def aggregate(self, all_aspects: List[List[AspectSentiment]]) -> Dict[str, Dict]:
        """èšåˆæ‰€æœ‰è¯„è®ºçš„æ–¹é¢åˆ†æ"""
        stats = defaultdict(lambda: {
            'positive': 0, 'negative': 0, 'neutral': 0,
            'total': 0, 'confidences': []
        })
        
        for review_aspects in all_aspects:
            for asp in review_aspects:
                key = asp.aspect
                stats[key][asp.sentiment.value] += 1
                stats[key]['total'] += 1
                stats[key]['confidences'].append(asp.confidence)
        
        result = {}
        for aspect, data in stats.items():
            total = data['total']
            if total == 0:
                continue
            
            result[aspect] = {
                'aspect_cn': self.ASPECTS[aspect]['cn'],
                'total_mentions': total,
                'positive_ratio': data['positive'] / total,
                'negative_ratio': data['negative'] / total,
                'neutral_ratio': data['neutral'] / total,
                'sentiment_score': (data['positive'] - data['negative']) / total,
                'avg_confidence': np.mean(data['confidences'])
            }
        
        return result


class TopicModeler:
    """
    ä¸»é¢˜å»ºæ¨¡å™¨
    
    æˆ‘ä½¿ç”¨çš„æ–¹æ³•æ˜¯: NMF 
    ç‰¹ç‚¹: æ¯” LDA æ›´é€‚åˆçŸ­æ–‡æœ¬ï¼Œä¸»é¢˜æ›´å¯è§£é‡Š
    """
    
    def __init__(self, n_topics: int = 8):
        self.n_topics = n_topics
        self.preprocessor = TextPreprocessor()
        self.vectorizer = None
        self.model = None
        self.feature_names = None
        
        # ä¸»é¢˜æ ‡ç­¾æ˜ å°„ (åŸºäºå…³é”®è¯è‡ªåŠ¨æ¨æ–­)
        self.topic_label_rules = {
            ('plot', 'story', 'twist'): 'å‰§æƒ…è½¬æŠ˜',
            ('acting', 'actor', 'performance'): 'æ¼”å‘˜æ¼”æŠ€',
            ('visual', 'effect', 'cgi'): 'è§†è§‰ç‰¹æ•ˆ',
            ('music', 'score', 'soundtrack'): 'èƒŒæ™¯éŸ³ä¹',
            ('director', 'nolan', 'vision'): 'å¯¼æ¼”é£æ ¼',
            ('pace', 'slow', 'boring'): 'èŠ‚å¥æ§åˆ¶',
            ('character', 'development', 'depth'): 'è§’è‰²å¡‘é€ ',
            ('ending', 'end', 'conclusion'): 'ç»“å±€',
            ('dialogue', 'line', 'script'): 'å¯¹ç™½å‰§æœ¬',
            ('emotion', 'feel', 'heart'): 'æƒ…æ„Ÿå…±é¸£'
        }
    
    def fit(self, documents: List[str]) -> bool:
        """è®­ç»ƒä¸»é¢˜æ¨¡å‹"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.decomposition import NMF
        
        # é¢„å¤„ç†
        processed = []
        for doc in documents:
            if doc and isinstance(doc, str) and len(doc) > 20:
                tokens = self.preprocessor.tokenize(self.preprocessor.clean(doc))
                if tokens:
                    processed.append(' '.join(tokens))
        
        if len(processed) < self.n_topics * 2:
            logger.warning(f"æ–‡æ¡£æ•°é‡ä¸è¶³: {len(processed)}")
            return False
        
        try:
            # TF-IDF å‘é‡åŒ–
            self.vectorizer = TfidfVectorizer(
                max_features=2000,
                min_df=3,
                max_df=0.85,
                ngram_range=(1, 2)
            )
            tfidf_matrix = self.vectorizer.fit_transform(processed)
            self.feature_names = self.vectorizer.get_feature_names_out()
            
            # NMF åˆ†è§£
            actual_n = min(self.n_topics, len(processed) - 1, tfidf_matrix.shape[1] - 1)
            self.model = NMF(
                n_components=actual_n,
                random_state=42,# å›ºå®šéšæœºç§å­ï¼å¯ä»¥ä¿è¯æ¯æ¬¡æ¼”ç¤ºç»“æœä¸€æ ·ï¼
                max_iter=200,
                init='nndsvd'# ä¸“é—¨ä¼˜åŒ–çš„åˆå§‹åŒ–æ–¹æ³•
            )
            self.doc_topics = self.model.fit_transform(tfidf_matrix)
            
            logger.info(f"âœ“ ä¸»é¢˜æ¨¡å‹è®­ç»ƒå®Œæˆ: {actual_n} ä¸ªä¸»é¢˜")
            return True
            
        except Exception as e:
            logger.error(f"ä¸»é¢˜å»ºæ¨¡å¤±è´¥: {e}")
            return False
    
    def _generate_label(self, keywords: List[str]) -> str:
        """æ ¹æ®å…³é”®è¯ç”Ÿæˆä¸»é¢˜æ ‡ç­¾"""#å¯å‘å¼#è¯­ä¹‰æ˜ å°„
        keywords_set = set(kw.lower() for kw in keywords[:5])
        
        best_match = None
        best_score = 0
        
        for rule_keywords, label in self.topic_label_rules.items():
            score = len(keywords_set & set(rule_keywords))
            if score > best_score:
                best_score = score
                best_match = label
        
        if best_match:
            return best_match
        return ', '.join(keywords[:3])
    
    def get_topics(self, n_words: int = 10) -> List[TopicInfo]:
        """è·å–ä¸»é¢˜ä¿¡æ¯"""
        if self.model is None:
            return []
        
        topics = []
        for idx, topic_weights in enumerate(self.model.components_):
            top_indices = topic_weights.argsort()[:-n_words-1:-1]
            keywords = [self.feature_names[i] for i in top_indices]
            weights = [float(topic_weights[i]) for i in top_indices]
            
            # è®¡ç®—è¯¥ä¸»é¢˜çš„æ–‡æ¡£æ•°
            topic_docs = (self.doc_topics[:, idx] > 0.1).sum()
            # ä¸‹ä¸€æ­¥ï¼šå°è£…æˆ TopicInfo å¯¹è±¡è¿”å›ï¼
            topics.append(TopicInfo(
                topic_id=idx,
                keywords=keywords,
                keyword_weights=weights,
                num_docs=int(topic_docs),
                label=self._generate_label(keywords)
            ))
        
        # æŒ‰æ–‡æ¡£æ•°æ’åº
        topics.sort(key=lambda x: x.num_docs, reverse=True)
        return topics
    
    def get_document_topics(self) -> np.ndarray:
        """è·å–æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ"""
        return self.doc_topics if hasattr(self, 'doc_topics') else None


class ReviewQualityScorer:
    """è¯„è®ºè´¨é‡è¯„åˆ†å™¨"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
    
    def score(self, text: str) -> Dict:
        """è¯„ä¼°è¯„è®ºè´¨é‡ (0-1)"""
        if not text or len(text) < 20:
            return {'overall': 0.0, 'details': {}}
        
        scores = {}
        
        # 1. é•¿åº¦åˆ†æ•° (100-800 å­—ç¬¦æœ€ä½³)
        length = len(text)
        if length < 50:
            scores['length'] = length / 50
        elif length <= 800:
            scores['length'] = 1.0
        else:
            scores['length'] = max(0.5, 1.0 - (length - 800) / 2000)
        
        # 2. è¯æ±‡å¤šæ ·æ€§
        tokens = self.preprocessor.tokenize(text, remove_stopwords=True)
        if tokens:
            unique_ratio = len(set(tokens)) / len(tokens)
            scores['diversity'] = min(unique_ratio * 1.3, 1.0)
        else:
            scores['diversity'] = 0.0
        
        # 3. å¥å­ç»“æ„
        sentences = self.preprocessor.extract_sentences(text)
        if sentences:
            avg_len = np.mean([len(s.split()) for s in sentences])
            if 8 <= avg_len <= 25:
                scores['structure'] = 1.0
            elif avg_len < 8:
                scores['structure'] = avg_len / 8
            else:
                scores['structure'] = max(0.3, 1 - (avg_len - 25) / 30)
        else:
            scores['structure'] = 0.3
        
        # 4. æœ‰å®è´¨å†…å®¹ (ä¸åªæ˜¯æ„Ÿå¹è¯)
        content_words = [t for t in tokens if len(t) > 3]
        scores['substance'] = min(len(content_words) / 10, 1.0)
        
        # ç»¼åˆè¯„åˆ†
        weights = {'length': 0.2, 'diversity': 0.3, 'structure': 0.2, 'substance': 0.3}
        overall = sum(scores[k] * weights[k] for k in weights)
        
        return {'overall': round(overall, 3), 'details': scores}


class NLPPipeline:
    """
    NLP åˆ†ææµæ°´çº¿
    æ•´åˆæ‰€æœ‰åˆ†æåŠŸèƒ½
    """
    
    def __init__(self, use_transformer: bool = True, n_topics: int = 8):
        logger.info("åˆå§‹åŒ– NLP Pipeline...")
        self.preprocessor = TextPreprocessor()
        self.sentiment_analyzer = MultiModelSentimentAnalyzer(use_transformer=use_transformer)
        self.aspect_analyzer = AspectBasedAnalyzer()
        self.topic_modeler = TopicModeler(n_topics=n_topics)
        self.quality_scorer = ReviewQualityScorer()
        logger.info("âœ“ NLP Pipeline åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_review(self, text: str, review_id: str = "") -> ReviewAnalysis:
        """åˆ†æå•æ¡è¯„è®º"""
        sentiment = self.sentiment_analyzer.analyze(text)
        aspects = self.aspect_analyzer.analyze(text)
        quality = self.quality_scorer.score(text)
        
        return ReviewAnalysis(
            review_id=review_id,
            sentiment=sentiment,
            aspects=aspects,
            quality_score=quality['overall'],
            topics=[],
            entities=[],
            word_count=len(text.split())
        )
    
    def process_dataframe(self, df: pd.DataFrame, text_column: str = 'content',
                         id_column: str = 'review_id',
                         run_topics: bool = True) -> pd.DataFrame:
        """å¤„ç† DataFrame"""
        from tqdm import tqdm
        
        df = df.copy()
        texts = df[text_column].fillna('').tolist()
        ids = df[id_column].tolist() if id_column in df.columns else range(len(df))
        
        # æƒ…æ„Ÿåˆ†æ
        logger.info("æ‰§è¡Œæƒ…æ„Ÿåˆ†æ...")
        sentiments = self.sentiment_analyzer.batch_analyze(texts)
        df['sentiment_label'] = [s.label.value for s in sentiments]
        df['sentiment_score'] = [s.score for s in sentiments]
        df['sentiment_confidence'] = [s.confidence for s in sentiments]
        
        # æ–¹é¢åˆ†æ
        logger.info("æ‰§è¡Œæ–¹é¢åˆ†æ...")
        aspects_list = []
        for text in tqdm(texts, desc="ABSA"):
            aspects_list.append(self.aspect_analyzer.analyze(text))
        df['aspects'] = aspects_list
        
        # è´¨é‡è¯„åˆ†
        logger.info("æ‰§è¡Œè´¨é‡è¯„åˆ†...")
        qualities = [self.quality_scorer.score(t) for t in tqdm(texts, desc="è´¨é‡è¯„åˆ†")]
        df['quality_score'] = [q['overall'] for q in qualities]
        
        # ä¸»é¢˜å»ºæ¨¡
        if run_topics:
            logger.info("æ‰§è¡Œä¸»é¢˜å»ºæ¨¡...")
            if self.topic_modeler.fit(texts):
                doc_topics = self.topic_modeler.get_document_topics()
                if doc_topics is not None and len(doc_topics) == len(df):
                    df['topic_id'] = doc_topics.argmax(axis=1)
                    df['topic_confidence'] = doc_topics.max(axis=1)
        
        return df
    
    def get_summary(self, df: pd.DataFrame) -> Dict:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            'total_reviews': len(df),
            'sentiment_distribution': df['sentiment_label'].value_counts().to_dict(),
            'positive_ratio': (df['sentiment_label'] == 'positive').mean(),
            'avg_sentiment_score': df['sentiment_score'].mean(),
            'avg_quality_score': df['quality_score'].mean(),
        }
        
        # æ–¹é¢ç»Ÿè®¡
        if 'aspects' in df.columns:
            summary['aspect_summary'] = self.aspect_analyzer.aggregate(df['aspects'].tolist())
        
        # ä¸»é¢˜ç»Ÿè®¡
        if self.topic_modeler.model is not None:
            summary['topics'] = [
                {'id': t.topic_id, 'label': t.label, 
                 'keywords': t.keywords[:5], 'num_docs': t.num_docs}
                for t in self.topic_modeler.get_topics()
            ]
        
        return summary
